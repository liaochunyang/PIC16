{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping\n",
    "\n",
    "## What is Web Scraping?\n",
    "\n",
    "- \"The web\": a collection of files hosted on a large network of \n",
    "communicating servers.\n",
    "- *Webscraping* : the act of accessing those files and programmatically saving them, or parts of them, to a chosen location (usually your computer). This is often a critical task  when writing projects that require\n",
    "data from the internet. \n",
    "\n",
    "\n",
    "\n",
    "HTML (HyperText Markup Language): said to be the fabric of the internet. \n",
    "\n",
    "Nearly all of the things that you \n",
    "would normally think of as \"webpages\" are really files \n",
    "written in HTML. A browser like Firefox, Chrome, or Safari is\n",
    "just a program for *rendering* HTML in an attractive visual \n",
    "format. \n",
    "\n",
    "- Unfortunately, for scraping, we often need to interact\n",
    "with raw HTML, which can get messy. \n",
    "- Fortunately, the BeautifulSoup package gives us some tools with which to do this. \n",
    "\n",
    "\n",
    "Resources:\n",
    "\n",
    "- pd.read_html: https://pandas.pydata.org/docs/reference/api/pandas.read_html.html\n",
    "\n",
    "- requests: https://requests.readthedocs.io/en/latest/\n",
    "\n",
    "- Introduction to HTML: https://www.w3schools.com/html/html_intro.asp\n",
    "\n",
    "- BeautifulSoup: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! conda install -c conda-forge beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the tutorial website we'll scrape from. \n",
    "\n",
    "http://quotes.toscrape.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are a number of quotes, which possess \n",
    "text, authors, and tags. There are multiple pages of \n",
    "these quotes, which are accessed via the \"Next\" button. \n",
    "\n",
    "For now, let's try just obtain the text on the webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "link = \"http://quotes.toscrape.com/\"\n",
    "data = requests.get(link).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `BeautifulSoup` type is a basis type for parsing a webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def link2soup(link):\n",
    "    \"\"\"Convert a link to a BeautifulSoup object.\"\"\"\n",
    "    data = requests.get(link).text\n",
    "    return BeautifulSoup(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup = link2soup(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS Selectors\n",
    "\n",
    "CSS (Cascaded Styling Sheet) is a file type for styling web pages. It is designed to apply some formatting to certain parts of the webpage. How do we select \"certain parts\"? That is what CSS selectors are for. \n",
    "\n",
    "\n",
    "- CSS selector references: https://www.w3schools.com/cssref/css_selectors.php\n",
    "- a fun activity: https://flukeout.github.io/\n",
    "\n",
    "\n",
    "A quick code to parse text, author name, and the list of tags:\n",
    "\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select(\"small.author\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup.select(\"small.author\")[0].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l = []\n",
    "\n",
    "for t in soup.select(\"div.quote\"):\n",
    "    text = t.select(\"span.text\")[0].get_text()\n",
    "    author = t.select(\"small.author\")[0].get_text()\n",
    "    tags = t.select(\"div.tags a.tag\")\n",
    "    tags = [x.get_text() for x in tags]\n",
    "    l.append((text, author, tags))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following the links\n",
    "\n",
    "At the bottom of each page, there is a \"next\" button. Can we follow the link?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next_button = soup.select(\".next a\")[0]\n",
    "next_button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next_url = link + next_button.attrs[\"href\"]\n",
    "next_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "next_soup = link2soup(next_url)\n",
    "next_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for t in next_soup.select(\"div.quote\"):\n",
    "    text = t.select(\"span.text\")[0].get_text()\n",
    "    author = t.select(\"small.author\")[0].get_text()\n",
    "    tags = t.select(\"div.tags a.tag\")\n",
    "    tags = [x.get_text() for x in tags]\n",
    "    l.append((text, author, tags))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Exercise__: Can we continue on and parse all the quotes on that website?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_page(l, soup, base_url):\n",
    "    \n",
    "    for t in soup.select(\"div.quote\"):\n",
    "        text = t.select(\"span.text\")[0].get_text()\n",
    "        author = t.select(\"small.author\")[0].get_text()\n",
    "        tags = t.select(\"div.tags a.tag\")\n",
    "        tags = [x.get_text() for x in tags]\n",
    "        l.append((text, author, tags))\n",
    "    next_button_match = soup.select(\".next a\")\n",
    "    \n",
    "    if next_button_match:\n",
    "        next_button = next_button_match[0]\n",
    "        next_url = base_url + next_button.attrs[\"href\"]\n",
    "        return next_url\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_url = \"http://quotes.toscrape.com/\"\n",
    "l = []\n",
    "soup = link2soup(base_url)\n",
    "while True:\n",
    "    next_url = parse_page(l, soup, base_url)\n",
    "    if not next_url:\n",
    "        break\n",
    "    else:\n",
    "        soup = link2soup(next_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Get the wikipedia links to country capitals\n",
    "\n",
    "Our question: \"*Get the Wikipedia links to each country capital from [this page](https://en.m.wikipedia.org/wiki/List_of_national_capitals)*\" (note the mobile page link)\n",
    "\n",
    "If you are on a desktop machine, the Wikipedia page has a table at the top before it goes into the table with the capitals/countries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup = link2soup(\"https://en.m.wikipedia.org/wiki/List_of_national_capitals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll('tr')[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recall that we have the hierarchy <tr> -> <td> -> <a>, and that the href= attribute is part of the <a> tag. We need to find all the <tr> tags, then get the (first) <td> tag for each, the <a> tag from the <td> tag, and finally get the href= from that.\n",
    "\n",
    "['https://en.m.wikipedia.org' + tr.td.a['href'] for tr in soup.findAll('tr')[1:]][:30]\n",
    "# (first 10 only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What if we want both the links for the capital AND the country? Then we need to get ALL the <td> tags for each <tr> row. Using a nested list comprehension(!):\n",
    "['https://en.m.wikipedia.org' + td.a['href'] for tr in soup.findAll('tr')[1:] \n",
    " for td in tr.findAll('td')[:2] if td.a][:30]\n",
    "# (first 10 only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: The 100 most popular feature films released in 2023\n",
    "\n",
    "Can be accessed at: https://www.imdb.com/search/title/?title_type=feature&release_date=2023-01-01,2023-12-31&count=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = \"https://www.imdb.com/search/title/?title_type=feature&release_date=2023-01-01,2023-12-31&count=100\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'} \n",
    "# you act like user, not a robot. \n",
    "data = requests.get(url, headers=headers).text\n",
    "soup = BeautifulSoup(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to scrape following 8 features from this page:\n",
    "- Rank (popularity)\n",
    "- Title\n",
    "- Description\n",
    "- Runtime\n",
    "- User rating\n",
    "- Metascore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('.ipc-title-link-wrapper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.select('.ipc-title-link-wrapper .ipc-title__text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank and title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_texts = [x.get_text() for x in soup.select('.ipc-title-link-wrapper .ipc-title__text')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re # the Python regex module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rank_data = [int(re.search('^[0-9]+', x).group(0)) for x in title_texts]\n",
    "rank_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_data = [x[re.search('^[0-9]+. ', x).end():] for x in title_texts]\n",
    "title_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "description_data = [x.get_text() for x in soup.select('.ipc-html-content-inner-div')]\n",
    "description_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_data = [x.get_text() for x in soup.select('.dli-title-metadata-item:nth-child(2)')]\n",
    "runtime_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_hr = [int(re.search(\"\\\\d+(?=h)\", x).group(0)) if re.search(\"\\\\d+(?=h)\", x) else 0 for x in runtime_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_min = [int(re.search(\"\\\\d+(?=m)\", x).group(0)) if re.search(\"\\\\d+(?=m)\", x) else 0 for x in runtime_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_data = [runtime_hr[i] * 60 + runtime_min[i] for i in range(len(runtime_hr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### User rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "userrating_data = [x.get_text() for x in soup.select('.ratingGroup--imdb-rating')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "userrating_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "userrating_data = [float(x.split('\\xa0')[0]) for x in userrating_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "userrating_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metascore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metascore_data = [float(x.get_text()) for x in soup.select('.metacritic-score-box')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(metascore_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, we only have 23 metascore data, and 2 are missing. How do we figure out the films with missing metascore?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixed = [x.get_text() for x in soup.select('.ipc-title-link-wrapper .ipc-title__text , .metacritic-score-box')]\n",
    "mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixed = [x[:re.search(' .*', x).start()] if re.search(' .*', x) else x for x in mixed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "isrank = np.array(['.' in x for x in mixed]) # indicates if the element is a rank\n",
    "ismissing=np.zeros(len(rank_data), dtype='bool')\n",
    "ismissing = isrank[:-1] & isrank[1:] # rank followed by another rank is missing metascore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ismissing = np.hstack([ismissing, [isrank[len(isrank)-1]]]) # check if the last entry is missing or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "missingpos = np.array([int(float(x)) for x in np.array(mixed)[np.array(ismissing)]]) - 1 # these are ranks missing metascore\n",
    "missingpos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.ones(25, dtype=bool)\n",
    "mask[missingpos] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metascore_data_ = np.zeros(25)\n",
    "metascore_data_[:] = np.nan\n",
    "metascore_data_[mask] = metascore_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metascore_data = metascore_data_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metascore_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data = {\n",
    "    \"poprank\" : rank_data,\n",
    "    \"title\" : title_data,\n",
    "    \"description\": description_data,\n",
    "    \"runtime\": runtime_data,\n",
    "    \"userrating\": userrating_data,\n",
    "    \"metascore\": metascore_data\n",
    "}\n",
    "                 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from plotly import express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df, \n",
    "                 x = \"userrating\",\n",
    "                 y = \"metascore\",\n",
    "                 hover_name = \"title\",\n",
    "                 height = 500,\n",
    "                 trendline=\"lowess\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
