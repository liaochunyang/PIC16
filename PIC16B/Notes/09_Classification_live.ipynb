{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "Classification problem is another supervised learning problem. The difference between regression and classification is the labels $y$. If $y$ can take any values, then it is a regression problem. If $y$ only takes discrete values, then it is a classification problem.\n",
    "\n",
    "\n",
    "# Classification example\n",
    "\n",
    "We can use the `make_classification` command to generate synthetic data samples.\n",
    "\n",
    "    sklearn.datasets.make_classification(n_samples=100, n_features=20, *, n_informative=2, n_redundant=2, n_repeated=0, n_classes=2, n_clusters_per_class=2, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbors\n",
    "\n",
    "#### Algorithm explanation:\n",
    "\n",
    "An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n",
    "\n",
    "#### Regression:\n",
    "K-nearest neighbors algorithm can also be used for regression problem. In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors. If k = 1, then the output is simply assigned to the value of that single nearest neighbor.\n",
    "\n",
    "#### Documentation:\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.74\n",
      "Test accuracy is 0.74\n",
      "[[0.625 0.125 0.125 0.125]\n",
      " [0.75  0.25  0.    0.   ]\n",
      " [0.    0.    1.    0.   ]\n",
      " ...\n",
      " [0.125 0.5   0.25  0.125]\n",
      " [0.625 0.    0.    0.375]\n",
      " [0.125 0.625 0.    0.25 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use cross-validation to select the best number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "See wikipedia for model explanation: https://en.wikipedia.org/wiki/Logistic_regression\n",
    "\n",
    "**Logistic regression is designed for classification problems only. In other words, you can not use this model for regression problems.**\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.59\n",
      "Test accuracy is 0.59\n",
      "[[0.56281303 0.04135957 0.07570028 0.32012711]\n",
      " [0.20904043 0.51212522 0.15466231 0.12417203]\n",
      " [0.67553186 0.12558172 0.16790161 0.03098482]\n",
      " ...\n",
      " [0.22122758 0.59547218 0.11085617 0.07244407]\n",
      " [0.79096746 0.02564928 0.00510108 0.17828218]\n",
      " [0.07873112 0.75625907 0.1178367  0.04717311]]\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree based model\n",
    "\n",
    "A decision tree looks like this: ![](https://scikit-learn.org/stable/_images/sphx_glr_plot_iris_dtc_002.png)\n",
    "\n",
    "\n",
    "A decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.76\n",
      "Test accuracy is 0.76\n",
      "[[0.65454545 0.         0.23636364 0.10909091]\n",
      " [0.75       0.         0.         0.25      ]\n",
      " [0.         0.         1.         0.        ]\n",
      " ...\n",
      " [0.03688525 0.91803279 0.03278689 0.01229508]\n",
      " [0.06976744 0.06976744 0.         0.86046512]\n",
      " [0.03688525 0.91803279 0.03278689 0.01229508]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Learning\n",
    "\n",
    "To better understand random forest model, we should know **Ensemble Learning** first, which is an important technique in Machine Learning.\n",
    "\n",
    "#### Motivation:\n",
    "\n",
    "Suppose that you have a complex question of thousands of random people, then aggregate their answers. In many cases you will find yjay yhis aggregated answer is better than an expert's answer. This is called the *wisdom of the crowd*. Similarly, if you aggregate the predictions of a groups of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of predictors is called an ensemble; thus this technique is called Ensemble Learning.\n",
    "\n",
    "#### When do we use ensemble model?\n",
    "\n",
    "Usually, ensemble model works better than single model, but there is no guarantee. In my opinion, since we must try different single models due to No Free Luch Theorem, it does not hurt to ensemble all single models you have tried, and look at the performance on test dataset.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Let's ensemble KNNeighbor, Logistic Regression, and Tree model together.\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier 0.7235\n",
      "LogisticRegression 0.588\n",
      "DecisionTreeClassifier 0.71\n",
      "VotingClassifier 0.741\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "In short, random forest is an ensemble of decision trees. \n",
    "\n",
    "A group of Decision Tree classifiers are trained on a different random subset of the training set. Then, you use max vote (ensemble technique) to obtain the prediction. \n",
    "\n",
    "\n",
    "Python documentation: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.8245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
